# Case description

[[_TOC_]]

**Disclaimer**: Neither the topic nor the data relates specifically to applied processes within ABN AMRO Bank N.V. The data provided is fake data generated by random draws from probability distributions. Neither the specific variables provided nor their probability distributions are indicative of data or processes within ABN AMRO Bank N.V.

Do you have any questions about this assessment? Please proceed with your own interpretation and write down your reasoning for making that interpretation.

To submit your solution, email the code and all accompanying files to saman.amini@nl.abnamro.com (please make sure that the zip file does not contain any binary content). Alternatively, you can also submit your code from a GitHub repository.

## Assessment
- You are part of the data engineering team at a bank. The bank collects vast amounts of transaction data from various sources daily. The bank has multiple datasets including transactions, accounts, and customers. Your task is to create a data pipeline that ingests this data, transforms it, and performs some basic analysis.


- The pipeline should be efficient, scalable, and written in production-ready code. Reusability and modularity are key aspects of the implementation.


- The objective of this assessment is to evaluate your proficiency in advanced data engineering tasks using Python, and PySpark while ensuring the solution is production-ready. This includes adhering to best practices for coding, configuration management, logging, error handling, and reusability, all structured in an OOP manner. 
Follow best practices for coding, including proper documentation and modular design.


- Provide clear documentation for each module and function.
Include a README file with instructions on how to run the pipeline and an explanation of the code structure.

## Evaluation Criteria:
- **Code Quality**: Adherence to best practices, readability, modularity, and reusability.
- **Functionality**: Correctness of the ingestion, transformation, and analysis processes.
- **Scalability**: Ability to handle large datasets efficiently.
- **Error Handling**: Robustness of the code with appropriate error handling and logging.
- **Testing**: Coverage and quality of unit tests.
- **Documentation**: Clarity and completeness of the documentation.


## Use-Case
You work for a bank that needs to process and analyze transaction data to generate monthly reports and insights. The bank has multiple datasets including transactions, accounts, and customers. Your task is to create a production-ready pipeline that ingests, transforms, and analyzes these datasets.

## Tasks
### Data Ingestion:
    - Load the datasets using Pandas.
    - Convert the Pandas dataframes to Spark dataframes.
    - Ensure the solution has proper error handling.

### Data Transformation:
    - Clean the data by handling missing or null values.
    - Convert date columns to datetime objects.
    - Create a new column in the transactions dataset named `year_month` that contains the year and month of the transaction in the format 'YYYY-MM'.
    - Merge the transactions dataset with the accounts dataset to include account details.
    - Merge the transactions dataset with the customers dataset to include customer details.

### Data Analysis:
    - Calculate the total deposit and withdrawal amount for each account for each `year_month`.
    - Calculate the total balance for each account type for each `year_month`.
    - Calculate the average transaction amount per customer.

### Data Export:
    - Save the aggregated data from PySpark to delta files.

### Data Enrichment:
    - Enrich the transactions dataset by adding a new column that classifies transactions as either 'high', 'medium', or 'low' based on the transaction amount. Save the final results in a separate file.
        - 'high' if the transaction amount is greater than $1000
        - 'medium' if the transaction amount is between $500 and $1000 (inclusive)
        - 'low' if the transaction amount is less than $500
    - Create an extra aggregation on the number of transactions per classification per week and save the results in a separate file.
    - Show the transactions with the largest amount per classification.

### Logging:
    - Implement logging to capture key events and errors.

### Configuration Management:
    - Use a configuration file (e.g., YAML or JSON) for setting parameters like file paths.

### Error Handling:
    - Implement robust error handling to ensure the pipeline is resilient to failures.

### Code Quality:
    - Write production-ready and reusable code.
    - Follow best practices for coding, including proper documentation and modular design.
    - Use a code styling tool like Ruff.

## Data
Transaction dataset (transactions.csv)
- `transaction_id` (string): Unique identifier for each transaction
- `account_id` (string): Unique identifier for each bank account
- `transaction_date` (string, format 'YYYY-MM-DD'): Date of the transaction
- `amount` (float): The amount of the transaction
- `transaction_type` (string): Type of the transaction (e.g., 'deposit', 'withdrawal')

Accounts Dataset (`accounts.csv`):
- `account_id` (string): Unique identifier for each bank account
- `customer_id` (string): Unique identifier for each customer
- `account_type` (string): Type of the account (e.g., 'savings', 'checking')
- `balance` (float): Current balance of the account

Customers Dataset (`customers.csv`):
- `customer_id` (string): Unique identifier for each customer
- `customer_name` (string): Name of the customer
- `join_date` (string, format 'YYYY-MM-DD'): Date the customer joined
